{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# H2 (Sparse users) refers to users with very little history (few interactions in the train).\n",
        "For them, CF (collaborative filtering) is almost \"blind,\" so content embeddings can be tried."
      ],
      "metadata": {
        "id": "Cj1tx64AhgM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's worth noting that we previously used the up0.01_ip0.01 slice, which identifies the 1% most active users and 1% most popular items. Now we want to look at sparse users, and we don't want to include active users because the goal is diametrically opposed. More specifically, to study the feasibility of recommending inactive users through content embedding, the optimal option is to use a different slice—ur0.01_ip0.01.\n",
        "\n",
        "Users — random (sparse)\n",
        "\n",
        "Items — popular\n",
        "\n",
        "What this does:\n",
        "\n",
        "Isolates the effect of sparse users\n",
        "\n",
        "Removes cold-start items\n",
        "\n",
        "Content works \"purely\" through the user\n",
        "\n",
        "Excellent if you want to clearly test H2 without mixing it with H3 (helps isolate the necessary hypothesis)."
      ],
      "metadata": {
        "id": "b1NHqW43k84l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 0) Installing dependencies (in one block)\n",
        "# ---------------------------\n",
        "!pip -q install -U pip setuptools wheel\n",
        "!pip -q install lightfm-next tqdm\n",
        "\n",
        "import numpy as np\n",
        "import polars as pl\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "from lightfm import LightFM\n",
        "from scipy import sparse\n",
        "\n",
        "from tqdm.auto import trange\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "PY84I5sL171u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "#Take ~10% of users from a subsample without killing RAM because ur0.01_ip0.01 is 10 times larger than up0.001_ip0.001\n",
        "# we work with the help of polars and thus cut from the cut (0)-(0)\n",
        "# ===========================\n",
        "# -------------------------\n",
        "# 0) Parameters\n",
        "# -------------------------\n",
        "subsample_name = \"ur0.01_ip0.01\"\n",
        "content_embedding_size = 32\n",
        "\n",
        "FRACTION_USERS = 0.10   # 10% of users from ur0.01_ip0.01\n",
        "SEED = 42\n",
        "\n",
        "TRAIN_WEEKS = list(range(25))\n",
        "VAL_WEEK = 25\n",
        "\n",
        "NEEDED_COLS = [\"user_id\", \"item_id\", \"timespent\", \"like\", \"share\", \"bookmark\"]\n",
        "\n",
        "# -------------------------\n",
        "# 1)Downloading files\n",
        "# -------------------------\n",
        "train_interactions_files = [\n",
        "    f\"subsamples/{subsample_name}/train/week_{i:02}.parquet\" for i in TRAIN_WEEKS\n",
        "]\n",
        "val_interactions_file = [f\"subsamples/{subsample_name}/validation/week_{VAL_WEEK:02}.parquet\"]\n",
        "\n",
        "metadata_files = [\n",
        "    \"metadata/users_metadata.parquet\",\n",
        "    \"metadata/items_metadata.parquet\",\n",
        "    \"metadata/item_embeddings.npz\",\n",
        "]\n",
        "\n",
        "for file in (train_interactions_files + val_interactions_file + metadata_files):\n",
        "    hf_hub_download(\n",
        "        repo_id=\"deepvk/VK-LSVD\",\n",
        "        repo_type=\"dataset\",\n",
        "        filename=file,\n",
        "        local_dir=\"VK-LSVD\",\n",
        "    )\n",
        "\n",
        "# -------------------------\n",
        "# 2) We lazily read train and select 10% of users\n",
        "# -------------------------\n",
        "train_lf = pl.concat([\n",
        "    pl.scan_parquet(f\"VK-LSVD/{file}\").select(NEEDED_COLS)\n",
        "    for file in train_interactions_files\n",
        "])\n",
        "\n",
        "# count the number of unique users (cheap)\n",
        "n_users = (\n",
        "    train_lf\n",
        "    .select(pl.col(\"user_id\").n_unique().alias(\"n_users\"))\n",
        "    .collect(engine=\"streaming\")[\"n_users\"][0]\n",
        ")\n",
        "n_sample = max(1, int(n_users * FRACTION_USERS))\n",
        "\n",
        "print(\"Unique users in subsample:\", int(n_users))\n",
        "print(\"Sampling users:\", int(n_sample), f\"({FRACTION_USERS*100:.1f}% users)\")\n",
        "\n",
        "# unique_users - a small table (safe to build)\n",
        "unique_users = (\n",
        "    train_lf\n",
        "    .select(\"user_id\")\n",
        "    .unique()\n",
        "    .collect(engine=\"streaming\")\n",
        ")\n",
        "\n",
        "# sampled_users — DataFrame\n",
        "sampled_users = unique_users.sample(n=n_sample, seed=SEED)\n",
        "\n",
        "# IMPORTANT: When joining with a LazyFrame, there must also be a LazyFrame on the right\n",
        "sampled_users_lf = sampled_users.lazy()\n",
        "\n",
        "# filter the train by these users (semi join)\n",
        "train_interactions = (\n",
        "    train_lf\n",
        "    .join(sampled_users_lf, on=\"user_id\", how=\"semi\")\n",
        "    .collect(engine=\"streaming\")\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# 3) Validation (filtering by the same users)\n",
        "# -------------------------\n",
        "val_lf = (\n",
        "    pl.scan_parquet(f\"VK-LSVD/{val_interactions_file[0]}\")\n",
        "    .select(NEEDED_COLS)\n",
        ")\n",
        "\n",
        "val_interactions = (\n",
        "    val_lf\n",
        "    .join(sampled_users_lf, on=\"user_id\", how=\"semi\")\n",
        "    .collect(engine=\"streaming\")\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# 4) Metadata and embeddings (filter by items from train)\n",
        "# -------------------------\n",
        "# users_metadata — a regular DataFrame, here you can do a join DataFrame ↔ DataFrame\n",
        "users_metadata = (\n",
        "    pl.read_parquet(\"VK-LSVD/metadata/users_metadata.parquet\")\n",
        "    .join(sampled_users, on=\"user_id\", how=\"inner\")\n",
        ")\n",
        "\n",
        "items_metadata_full = pl.read_parquet(\"VK-LSVD/metadata/items_metadata.parquet\")\n",
        "\n",
        "train_items = train_interactions.select(\"item_id\").unique()\n",
        "\n",
        "items_metadata = items_metadata_full.join(train_items, on=\"item_id\", how=\"inner\")\n",
        "items_duration = items_metadata.select([\"item_id\", \"duration\"])\n",
        "\n",
        "# embeddings\n",
        "npz = np.load(\"VK-LSVD/metadata/item_embeddings.npz\")\n",
        "item_ids_all = npz[\"item_id\"]\n",
        "item_emb_all = npz[\"embedding\"].astype(np.float32)\n",
        "\n",
        "train_items_np = train_items[\"item_id\"].to_numpy()\n",
        "mask_items = np.isin(item_ids_all, train_items_np)\n",
        "\n",
        "item_ids = item_ids_all[mask_items]\n",
        "item_embeddings = item_emb_all[mask_items][:, :content_embedding_size]\n",
        "\n",
        "# -------------------------\n",
        "# 5) Final diagnostics\n",
        "# -------------------------\n",
        "print(\"\\nLoaded subsample:\", subsample_name)\n",
        "print(\"Train rows (filtered):\", train_interactions.height)\n",
        "print(\"Val rows (filtered):\", val_interactions.height)\n",
        "print(\"Train users (sampled):\", sampled_users.height)\n",
        "print(\"Train items:\", train_items.height)\n",
        "print(\"Embeddings items:\", len(item_ids), \"Embedding dim:\", item_embeddings.shape[1])\n",
        "print(\"Users metadata rows:\", users_metadata.height)\n",
        "print(\"Items metadata rows:\", items_metadata.height)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NcZmveolsF-",
        "outputId": "478e95a8-584a-4fe3-b775-48af8d6e36f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique users in subsample: 99805\n",
            "Sampling users: 9980 (10.0% users)\n",
            "\n",
            "Loaded subsample: ur0.01_ip0.01\n",
            "Train rows (filtered): 18183542\n",
            "Val rows (filtered): 535276\n",
            "Train users (sampled): 9980\n",
            "Train items: 196277\n",
            "Embeddings items: 196277 Embedding dim: 32\n",
            "Users metadata rows: 9980\n",
            "Items metadata rows: 196277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sparse users are defined as users with a small number of interactions\n",
        "within a randomly sampled subset of users from the original dataset.\n",
        "\n",
        "Thus, sparsity is relative to the sampled population rather than the\n",
        "full dataset.\n"
      ],
      "metadata": {
        "id": "poVu26SiXSlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Training and evaluation of H2 (Sparse users)\n",
        "# =========================\n",
        "\n",
        "# ---------- 1) Positives ----------\n",
        "def add_watch_ratio(df: pl.DataFrame, items_duration: pl.DataFrame) -> pl.DataFrame:\n",
        "    return (\n",
        "        df.select([\"user_id\", \"item_id\", \"timespent\"])\n",
        "          .join(items_duration, on=\"item_id\", how=\"inner\")\n",
        "          .with_columns([\n",
        "              (pl.col(\"timespent\").cast(pl.Float32) / pl.col(\"duration\").cast(pl.Float32))\n",
        "              .clip(0.0, 10.0)\n",
        "              .alias(\"watch_ratio\")\n",
        "          ])\n",
        "    )\n",
        "\n",
        "WATCH_THR = 0.5\n",
        "train_w = add_watch_ratio(train_interactions, items_duration)\n",
        "val_w   = add_watch_ratio(val_interactions, items_duration)\n",
        "\n",
        "train_pos_P1 = train_w.filter(pl.col(\"watch_ratio\") >= WATCH_THR).select([\"user_id\", \"item_id\"])\n",
        "val_pos_P1   = val_w.filter(pl.col(\"watch_ratio\") >= WATCH_THR).select([\"user_id\", \"item_id\"])\n",
        "\n",
        "train_pos_P2 = train_interactions.filter(pl.col(\"like\") | pl.col(\"share\") | pl.col(\"bookmark\")).select([\"user_id\", \"item_id\"])\n",
        "val_pos_P2   = val_interactions.filter(pl.col(\"like\") | pl.col(\"share\") | pl.col(\"bookmark\")).select([\"user_id\", \"item_id\"])\n",
        "\n",
        "\n",
        "# ---------- 2) mapping ----------\n",
        "def build_mappings(train_pos: pl.DataFrame):\n",
        "    user_ids = train_pos.select(\"user_id\").unique().sort(\"user_id\")[\"user_id\"].to_numpy()\n",
        "    item_ids_train = train_pos.select(\"item_id\").unique().sort(\"item_id\")[\"item_id\"].to_numpy()\n",
        "    user2idx = {int(u): i for i, u in enumerate(user_ids)}\n",
        "    item2idx = {int(it): i for i, it in enumerate(item_ids_train)}\n",
        "    return user_ids, item_ids_train, user2idx, item2idx\n",
        "\n",
        "\n",
        "# ---------- 3) sparse interactions ----------\n",
        "def to_sparse_matrix(pos_df: pl.DataFrame, user2idx, item2idx, n_users, n_items):\n",
        "    u = pos_df[\"user_id\"].to_numpy()\n",
        "    it = pos_df[\"item_id\"].to_numpy()\n",
        "\n",
        "    u_idx = np.fromiter((user2idx.get(int(x), -1) for x in u), dtype=np.int32, count=len(u))\n",
        "    it_idx = np.fromiter((item2idx.get(int(x), -1) for x in it), dtype=np.int32, count=len(it))\n",
        "\n",
        "    mask = (u_idx >= 0) & (it_idx >= 0)\n",
        "    u_idx = u_idx[mask]\n",
        "    it_idx = it_idx[mask]\n",
        "\n",
        "    mat = sparse.coo_matrix(\n",
        "        (np.ones(len(u_idx), dtype=np.float32), (u_idx, it_idx)),\n",
        "        shape=(n_users, n_items)\n",
        "    ).tocsr()\n",
        "\n",
        "    # if there were duplicate pairs, convert to binary form\n",
        "    if mat.nnz > 0:\n",
        "        mat.data[:] = 1.0\n",
        "    return mat\n",
        "\n",
        "\n",
        "# ---------- 4) sparse users (H2) ----------\n",
        "def get_sparse_users(train_mat, bottom_quantile=0.3):\n",
        "    user_counts = np.array(train_mat.getnnz(axis=1)).astype(np.int32)\n",
        "    thr = float(np.quantile(user_counts, bottom_quantile))\n",
        "    sparse_idx = np.where(user_counts <= thr)[0]\n",
        "    return sparse_idx, user_counts, thr\n",
        "\n",
        "\n",
        "# ---------- 5) item_features from embeddings----------\n",
        "def build_item_features(item_ids_train, item2idx, item_ids_emb, item_emb):\n",
        "    emb_map = {int(i): item_emb[j].astype(np.float32) for j, i in enumerate(item_ids_emb)}\n",
        "    n_items = len(item_ids_train)\n",
        "    n_feat = item_emb.shape[1]\n",
        "\n",
        "    rows, cols, data = [], [], []\n",
        "    for it_id, it_i in item2idx.items():\n",
        "        emb = emb_map.get(int(it_id))\n",
        "        if emb is None:\n",
        "            continue\n",
        "# convert D-dimensional embedding into D \"features\"\n",
        "# (LightFM expects a sparse item_features matrix)\n",
        "        for f in range(n_feat):\n",
        "            val = float(emb[f])\n",
        "            if val != 0.0:\n",
        "                rows.append(it_i)\n",
        "                cols.append(f)\n",
        "                data.append(val)\n",
        "\n",
        "    feats = sparse.coo_matrix((data, (rows, cols)), shape=(n_items, n_feat), dtype=np.float32).tocsr()\n",
        "    return feats\n",
        "\n",
        "\n",
        "# ---------- 6) metrics ----------\n",
        "def ndcg_at_k(recs, gt_set, k=10):\n",
        "    dcg = 0.0\n",
        "    for rank, it in enumerate(recs[:k], start=1):\n",
        "        if it in gt_set:\n",
        "            dcg += 1.0 / np.log2(rank + 1)\n",
        "    ideal = sum(1.0 / np.log2(r + 1) for r in range(1, min(len(gt_set), k) + 1))\n",
        "    return dcg / ideal if ideal > 0 else 0.0\n",
        "\n",
        "def recall_at_k(recs, gt_set, k=10):\n",
        "    if len(gt_set) == 0:\n",
        "        return 0.0\n",
        "    hit = sum(1 for it in recs[:k] if it in gt_set)\n",
        "    return hit / len(gt_set)\n",
        "\n",
        "\n",
        "# ---------- 7) evaluation (fix predict) ----------\n",
        "def evaluate_model(model, train_mat, val_mat, user_idx, item_features=None,\n",
        "                   k=10, max_users=2000, seed=42, num_threads=4):\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    users = np.array(user_idx, dtype=np.int32)\n",
        "    if len(users) > max_users:\n",
        "        users = rng.choice(users, size=max_users, replace=False)\n",
        "\n",
        "    ndcgs, recalls = [], []\n",
        "    all_items = np.arange(train_mat.shape[1], dtype=np.int32)\n",
        "\n",
        "    for u in users:\n",
        "        gt_items = val_mat[u].indices\n",
        "        if len(gt_items) == 0:\n",
        "            continue\n",
        "\n",
        "        # ✅ LightFM.predict requires user_ids and item_ids to be the same length\n",
        "        u_arr = np.full(len(all_items), int(u), dtype=np.int32)\n",
        "        scores = model.predict(u_arr, all_items, item_features=item_features,\n",
        "                               num_threads=num_threads)\n",
        "\n",
        "       # remove already seen\n",
        "        seen = train_mat[u].indices\n",
        "        scores[seen] = -1e9\n",
        "\n",
        "        topk = np.argpartition(-scores, k)[:k]\n",
        "        topk = topk[np.argsort(-scores[topk])]\n",
        "\n",
        "        gt_set = set(gt_items.tolist())\n",
        "        ndcgs.append(ndcg_at_k(topk.tolist(), gt_set, k=k))\n",
        "        recalls.append(recall_at_k(topk.tolist(), gt_set, k=k))\n",
        "\n",
        "    return float(np.mean(ndcgs)) if ndcgs else 0.0, float(np.mean(recalls)) if recalls else 0.0, len(ndcgs)\n",
        "\n",
        "\n",
        "def plot_history(hist, title_prefix=\"\"):\n",
        "    epochs = [h[\"epoch\"] for h in hist]\n",
        "    for key in [\"ndcg\", \"recall\"]:\n",
        "        vals = [h[key] for h in hist]\n",
        "        plt.figure()\n",
        "        plt.plot(epochs, vals)\n",
        "        plt.xlabel(\"epoch\")\n",
        "        plt.ylabel(key)\n",
        "        plt.title(f\"{title_prefix}{key}@K by epoch\")\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def train_with_history(train_mat, val_mat, sparse_idx, item_features=None,\n",
        "                       loss=\"bpr\", no_components=64, lr=0.05, epochs=5,\n",
        "                       k=10, max_users_eval=2000, num_threads=4, seed=42):\n",
        "    model = LightFM(loss=loss, no_components=no_components, learning_rate=lr, random_state=seed)\n",
        "    hist = []\n",
        "\n",
        "    desc = \"Hybrid\" if item_features is not None else \"CF-only\"\n",
        "    for ep in trange(1, epochs + 1, desc=f\"Training {desc}\"):\n",
        "        model.fit_partial(train_mat, item_features=item_features, epochs=1, num_threads=num_threads)\n",
        "\n",
        "        ndcg, rec, n_eval = evaluate_model(\n",
        "            model, train_mat, val_mat, sparse_idx,\n",
        "            item_features=item_features, k=k,\n",
        "            max_users=max_users_eval, seed=seed, num_threads=num_threads\n",
        "        )\n",
        "        hist.append({\"epoch\": ep, \"ndcg\": ndcg, \"recall\": rec, \"n_eval\": n_eval})\n",
        "\n",
        "    return model, hist\n",
        "\n",
        "# ---------- 8) Main Experiment H2 ----------\n",
        "def run_h2_experiment(train_pos, val_pos, label,\n",
        "                      bottom_quantile=0.3,\n",
        "                      no_components=64, epochs=5, lr=0.05, k=10,\n",
        "                      max_users_eval=2000, num_threads=4, seed=42):\n",
        "\n",
        "    print(f\"\\n================ H2 (Sparse Users): {label} ================\")\n",
        "\n",
        "    user_ids, item_ids_train, user2idx, item2idx = build_mappings(train_pos)\n",
        "    n_users, n_items = len(user_ids), len(item_ids_train)\n",
        "\n",
        "    val_pos_f = val_pos.filter(\n",
        "        pl.col(\"user_id\").is_in(pl.Series(user_ids)) &\n",
        "        pl.col(\"item_id\").is_in(pl.Series(item_ids_train))\n",
        "    )\n",
        "\n",
        "    train_mat = to_sparse_matrix(train_pos, user2idx, item2idx, n_users, n_items)\n",
        "    val_mat   = to_sparse_matrix(val_pos_f, user2idx, item2idx, n_users, n_items)\n",
        "\n",
        "    print(\"train:\", train_mat.shape, \"nnz:\", train_mat.nnz)\n",
        "    print(\"val:  \", val_mat.shape, \"nnz:\", val_mat.nnz)\n",
        "\n",
        "    sparse_idx, user_counts, thr = get_sparse_users(train_mat, bottom_quantile=bottom_quantile)\n",
        "    print(\"sparse users threshold (<=):\", thr)\n",
        "    print(\"sparse users:\", len(sparse_idx), \"/\", n_users)\n",
        "\n",
        "    item_features = build_item_features(item_ids_train, item2idx, item_ids, item_embeddings)\n",
        "    print(\"item_features:\", item_features.shape, \"nnz:\", item_features.nnz)\n",
        "\n",
        "    # CF-only\n",
        "    model_cf, hist_cf = train_with_history(\n",
        "        train_mat, val_mat, sparse_idx,\n",
        "        item_features=None, loss=\"bpr\",\n",
        "        no_components=no_components, lr=lr, epochs=epochs,\n",
        "        k=k, max_users_eval=max_users_eval, num_threads=num_threads, seed=seed\n",
        "    )\n",
        "\n",
        "    # Hybrid\n",
        "    model_h, hist_h = train_with_history(\n",
        "        train_mat, val_mat, sparse_idx,\n",
        "        item_features=item_features, loss=\"bpr\",\n",
        "        no_components=no_components, lr=lr, epochs=epochs,\n",
        "        k=k, max_users_eval=max_users_eval, num_threads=num_threads, seed=seed\n",
        "    )\n",
        "\n",
        "    ndcg_cf, rec_cf = hist_cf[-1][\"ndcg\"], hist_cf[-1][\"recall\"]\n",
        "    ndcg_h,  rec_h  = hist_h[-1][\"ndcg\"],  hist_h[-1][\"recall\"]\n",
        "\n",
        "    print(f\"CF-only    | NDCG@{k}: {ndcg_cf:.5f} | Recall@{k}: {rec_cf:.5f}\")\n",
        "    print(f\"CF+content | NDCG@{k}: {ndcg_h:.5f} | Recall@{k}: {rec_h:.5f}\")\n",
        "    delta = ndcg_h - ndcg_cf\n",
        "    print(f\"ΔNDCG@{k}: {delta:+.5f}\")\n",
        "\n",
        "    if delta >= 0:\n",
        "        print(\"H2 (part one) is confirmed: content helps sparse users.\")\n",
        "    else:\n",
        "        print(\"H2 is weakened: content does not help (or hinders) sparse users.\")\n",
        "\n",
        "    plot_history(hist_cf, title_prefix=f\"{label} | CF-only | \")\n",
        "    plot_history(hist_h,  title_prefix=f\"{label} | CF+content | \")\n",
        "\n",
        "    return {\n",
        "        \"label\": label,\n",
        "        \"bottom_quantile\": bottom_quantile,\n",
        "        \"hist_cf\": hist_cf,\n",
        "        \"hist_h\": hist_h,\n",
        "        \"ndcg_cf\": ndcg_cf, \"rec_cf\": rec_cf,\n",
        "        \"ndcg_h\": ndcg_h,   \"rec_h\": rec_h,\n",
        "        \"delta_ndcg\": float(delta),\n",
        "        \"n_users_train\": n_users,\n",
        "        \"n_items_train\": n_items,\n",
        "        \"n_sparse\": int(len(sparse_idx)),\n",
        "    }\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AOQBhWLwm-dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 9) Launching H2 on P1 and P2 ----------\n",
        "NUM_THREADS = 4\n",
        "EPOCHS = 5\n",
        "K = 10\n",
        "\n",
        "res_H2_P1 = run_h2_experiment(\n",
        "    train_pos_P1, val_pos_P1,\n",
        "    label=f\"H2/P1 (subsample={subsample_name}): watch_ratio >= {WATCH_THR}\",\n",
        "    bottom_quantile=0.3,\n",
        "    epochs=EPOCHS, k=K,\n",
        "    no_components=64, lr=0.05,\n",
        "    max_users_eval=2000, num_threads=NUM_THREADS\n",
        ")\n",
        "\n",
        "res_H2_P2 = run_h2_experiment(\n",
        "    train_pos_P2, val_pos_P2,\n",
        "    label=f\"H2/P2 (subsample={subsample_name}): like OR share OR bookmark\",\n",
        "    bottom_quantile=0.3,\n",
        "    epochs=EPOCHS, k=K,\n",
        "    no_components=64, lr=0.05,\n",
        "    max_users_eval=2000, num_threads=NUM_THREADS\n",
        ")\n",
        "\n",
        "res_H2_P1[\"delta_ndcg\"], res_H2_P2[\"delta_ndcg\"]\n"
      ],
      "metadata": {
        "id": "6oOdtiXgnOAp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}