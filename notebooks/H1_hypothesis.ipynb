{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJaDCTJmr1O6"
      },
      "source": [
        "# Exploring H1 (Dense Mode):\n",
        "Hypothesis: For active users, adding content embeddings does not improve NDCG@10 compared to CF-only and may worsen ranking due to signal conflict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30B_amgXYNXX"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# 0) Installing dependencies (in one block)\n",
        "# ---------------------------\n",
        "!pip -q install -U pip setuptools wheel\n",
        "!pip -q install lightfm-next tqdm\n",
        "\n",
        "import numpy as np\n",
        "import polars as pl\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "from lightfm import LightFM\n",
        "from scipy import sparse\n",
        "\n",
        "from tqdm.auto import trange\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8efGVbS7sftW"
      },
      "outputs": [],
      "source": [
        "subsample_name = 'up0.001_ip0.001'\n",
        "content_embedding_size = 32\n",
        "\n",
        "train_interactions_files = [f'subsamples/{subsample_name}/train/week_{i:02}.parquet'\n",
        "                            for i in range(25)]\n",
        "val_interactions_file = [f'subsamples/{subsample_name}/validation/week_25.parquet']\n",
        "\n",
        "metadata_files = ['metadata/users_metadata.parquet',\n",
        "                  'metadata/items_metadata.parquet',\n",
        "                  'metadata/item_embeddings.npz']\n",
        "\n",
        "for file in (train_interactions_files + val_interactions_file + metadata_files):\n",
        "    hf_hub_download(\n",
        "        repo_id='deepvk/VK-LSVD', repo_type='dataset',\n",
        "        filename=file, local_dir='VK-LSVD'\n",
        "    )\n",
        "\n",
        "train_interactions = pl.concat([pl.scan_parquet(f'VK-LSVD/{file}')\n",
        "                                for file in train_interactions_files]).collect(engine='streaming')\n",
        "\n",
        "val_interactions = pl.read_parquet(f'VK-LSVD/{val_interactions_file[0]}')\n",
        "\n",
        "train_users = train_interactions.select('user_id').unique()\n",
        "train_items = train_interactions.select('item_id').unique()\n",
        "\n",
        "item_ids = np.load('VK-LSVD/metadata/item_embeddings.npz')['item_id']\n",
        "item_embeddings = np.load('VK-LSVD/metadata/item_embeddings.npz')['embedding']\n",
        "\n",
        "mask = np.isin(item_ids, train_items.to_numpy())\n",
        "item_ids = item_ids[mask]\n",
        "item_embeddings = item_embeddings[mask][:, :content_embedding_size]\n",
        "\n",
        "users_metadata = pl.read_parquet('VK-LSVD/metadata/users_metadata.parquet').join(train_users, on='user_id')\n",
        "items_metadata = pl.read_parquet('VK-LSVD/metadata/items_metadata.parquet').join(train_items, on='item_id')\n",
        "\n",
        "# (optional) You added embedding as a column - we'll leave it as is:\n",
        "items_metadata = items_metadata.join(\n",
        "    pl.DataFrame({'item_id': item_ids, 'embedding': item_embeddings}),\n",
        "    on='item_id'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kbIPDxqshjE"
      },
      "outputs": [],
      "source": [
        "train_interactions.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwq-zDpf7o2N"
      },
      "outputs": [],
      "source": [
        "items_metadata.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IW6B08hslXU"
      },
      "source": [
        "Let's check the loaded volumes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVVIOPbKsolU"
      },
      "outputs": [],
      "source": [
        "print(\"train rows:\", train_interactions.height)\n",
        "print(\"train users:\", train_interactions.select(\"user_id\").n_unique())\n",
        "print(\"train items:\", train_interactions.select(\"item_id\").n_unique())\n",
        "\n",
        "print(\"val rows:\", val_interactions.height)\n",
        "print(\"val users:\", val_interactions.select(\"user_id\").n_unique())\n",
        "print(\"val items:\", val_interactions.select(\"item_id\").n_unique())\n",
        "\n",
        "print(\"embeddings items:\", len(item_ids))\n",
        "print(\"embedding dim:\", item_embeddings.shape[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_UDE8xdsu6R"
      },
      "source": [
        "47 million rows for 10,000 users is large enough to be analyzed using pandas, so we'll stick with Polars to handle such volumes. The slice features for the last 25 weeks, which are recorded as validation users, show that approximately the same number of users remains (9,904). This means that almost all users from the train database survived until validation, so there are few cold start users. This should be taken into account."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8YqhKJDYZKS"
      },
      "source": [
        "\n",
        "Positives P1/P2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvYaIEBaYUUx"
      },
      "outputs": [],
      "source": [
        "# duration (для watch_ratio)\n",
        "items_duration = items_metadata.select([\"item_id\", \"duration\"])\n",
        "\n",
        "def add_watch_ratio(df: pl.DataFrame, items_duration: pl.DataFrame) -> pl.DataFrame:\n",
        "    return (\n",
        "        df.select([\"user_id\", \"item_id\", \"timespent\"])\n",
        "          .join(items_duration, on=\"item_id\", how=\"inner\")\n",
        "          .with_columns([\n",
        "              (pl.col(\"timespent\").cast(pl.Float32) / pl.col(\"duration\").cast(pl.Float32))\n",
        "              .clip(0.0, 10.0)\n",
        "              .alias(\"watch_ratio\")\n",
        "          ])\n",
        "    )\n",
        "\n",
        "train_w = add_watch_ratio(train_interactions, items_duration)\n",
        "val_w   = add_watch_ratio(val_interactions, items_duration)\n",
        "\n",
        "WATCH_THR = 0.5\n",
        "train_pos_P1 = train_w.filter(pl.col(\"watch_ratio\") >= WATCH_THR).select([\"user_id\", \"item_id\"])\n",
        "val_pos_P1   = val_w.filter(pl.col(\"watch_ratio\") >= WATCH_THR).select([\"user_id\", \"item_id\"])\n",
        "\n",
        "train_pos_P2 = (\n",
        "    train_interactions\n",
        "    .filter(pl.col(\"like\") | pl.col(\"share\") | pl.col(\"bookmark\"))\n",
        "    .select([\"user_id\", \"item_id\"])\n",
        ")\n",
        "val_pos_P2 = (\n",
        "    val_interactions\n",
        "    .filter(pl.col(\"like\") | pl.col(\"share\") | pl.col(\"bookmark\"))\n",
        "    .select([\"user_id\", \"item_id\"])\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV6YkC0zYdRp"
      },
      "source": [
        "Mapping + sparse matrices + active users"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0b98FxugYgfJ"
      },
      "outputs": [],
      "source": [
        "def build_mappings(train_pos: pl.DataFrame):\n",
        "    user_ids = train_pos.select(\"user_id\").unique().sort(\"user_id\")[\"user_id\"].to_numpy()\n",
        "    item_ids_train = train_pos.select(\"item_id\").unique().sort(\"item_id\")[\"item_id\"].to_numpy()\n",
        "\n",
        "    user2idx = {int(u): i for i, u in enumerate(user_ids)}\n",
        "    item2idx = {int(it): i for i, it in enumerate(item_ids_train)}\n",
        "\n",
        "    return user_ids, item_ids_train, user2idx, item2idx\n",
        "\n",
        "def to_sparse_matrix(pos_df: pl.DataFrame, user2idx, item2idx, n_users, n_items):\n",
        "    u = pos_df[\"user_id\"].to_numpy()\n",
        "    it = pos_df[\"item_id\"].to_numpy()\n",
        "\n",
        "    u_idx = np.fromiter((user2idx.get(int(x), -1) for x in u), dtype=np.int32, count=len(u))\n",
        "    it_idx = np.fromiter((item2idx.get(int(x), -1) for x in it), dtype=np.int32, count=len(it))\n",
        "\n",
        "    mask = (u_idx >= 0) & (it_idx >= 0)\n",
        "    u_idx = u_idx[mask]\n",
        "    it_idx = it_idx[mask]\n",
        "\n",
        "    mat = sparse.coo_matrix(\n",
        "        (np.ones(len(u_idx), dtype=np.float32), (u_idx, it_idx)),\n",
        "        shape=(n_users, n_items)\n",
        "    ).tocsr()\n",
        "\n",
        "   # if there are duplicate pairs, we will convert them to 0/1:\n",
        "    if mat.nnz > 0:\n",
        "        mat.data[:] = 1.0\n",
        "\n",
        "    return mat\n",
        "\n",
        "def get_active_users(train_mat, top_quantile=0.7):\n",
        "    user_counts = np.array(train_mat.getnnz(axis=1)).astype(np.int32)\n",
        "    thr = float(np.quantile(user_counts, top_quantile))\n",
        "    active_idx = np.where(user_counts >= thr)[0]\n",
        "    return active_idx, user_counts, thr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaSSuOClYjNX"
      },
      "source": [
        "Item features from content embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7a5EwM7Yk8I"
      },
      "outputs": [],
      "source": [
        "def build_item_features(item_ids_train, item2idx, item_ids_emb, item_emb):\n",
        "    emb_map = {int(i): item_emb[j].astype(np.float32) for j, i in enumerate(item_ids_emb)}\n",
        "\n",
        "    n_items = len(item_ids_train)\n",
        "    n_feat = item_emb.shape[1]\n",
        "\n",
        "    rows, cols, data = [], [], []\n",
        "\n",
        "    for it_id, it_i in item2idx.items():\n",
        "        emb = emb_map.get(int(it_id))\n",
        "        if emb is None:\n",
        "            continue\n",
        "        for f in range(n_feat):\n",
        "            val = float(emb[f])\n",
        "            if val != 0.0:\n",
        "                rows.append(it_i)\n",
        "                cols.append(f)\n",
        "                data.append(val)\n",
        "\n",
        "    feats = sparse.coo_matrix(\n",
        "        (data, (rows, cols)),\n",
        "        shape=(n_items, n_feat),\n",
        "        dtype=np.float32\n",
        "    ).tocsr()\n",
        "\n",
        "    return feats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-L8pzNMQYnBz"
      },
      "source": [
        "Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Alsh0-F4Yo6o"
      },
      "outputs": [],
      "source": [
        "def ndcg_at_k(recs, gt_set, k=10):\n",
        "    dcg = 0.0\n",
        "    for rank, it in enumerate(recs[:k], start=1):\n",
        "        if it in gt_set:\n",
        "            dcg += 1.0 / np.log2(rank + 1)\n",
        "    ideal = sum(1.0 / np.log2(r + 1) for r in range(1, min(len(gt_set), k) + 1))\n",
        "    return dcg / ideal if ideal > 0 else 0.0\n",
        "\n",
        "def recall_at_k(recs, gt_set, k=10):\n",
        "    if len(gt_set) == 0:\n",
        "        return 0.0\n",
        "    hit = sum(1 for it in recs[:k] if it in gt_set)\n",
        "    return hit / len(gt_set)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s90o_PfWY4mZ"
      },
      "source": [
        "Model structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kz2515q2YvYQ"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# - evaluate_model\n",
        "# - plot_history\n",
        "# - train_lightfm_with_history\n",
        "# - run_h1_experiment (different seeds)\n",
        "# =========================\n",
        "def evaluate_model(model, train_mat, val_mat, user_idx, item_features=None,\n",
        "                   k=10, max_users=2000, seed=42, num_threads=4):\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    users = np.array(user_idx, dtype=np.int32)\n",
        "    if len(users) == 0:\n",
        "        return 0.0, 0.0, 0\n",
        "\n",
        "    if len(users) > max_users:\n",
        "        users = rng.choice(users, size=max_users, replace=False)\n",
        "\n",
        "    ndcgs, recalls = [], []\n",
        "    all_items = np.arange(train_mat.shape[1], dtype=np.int32)\n",
        "\n",
        "    for u in users:\n",
        "        gt_items = val_mat[u].indices\n",
        "        if len(gt_items) == 0:\n",
        "            continue\n",
        "\n",
        "        # LightFM.predict requires: len(user_ids) == len(item_ids)\n",
        "        u_arr = np.full(len(all_items), int(u), dtype=np.int32)\n",
        "        scores = model.predict(\n",
        "            u_arr, all_items,\n",
        "            item_features=item_features,\n",
        "            num_threads=num_threads\n",
        "        )\n",
        "\n",
        "        # exclude already seen in train\n",
        "        seen = train_mat[u].indices\n",
        "        scores[seen] = -1e9\n",
        "\n",
        "        topk = np.argpartition(-scores, k)[:k]\n",
        "        topk = topk[np.argsort(-scores[topk])]\n",
        "\n",
        "        gt_set = set(gt_items.tolist())\n",
        "        ndcgs.append(ndcg_at_k(topk.tolist(), gt_set, k=k))\n",
        "        recalls.append(recall_at_k(topk.tolist(), gt_set, k=k))\n",
        "\n",
        "    if len(ndcgs) == 0:\n",
        "        return 0.0, 0.0, 0\n",
        "\n",
        "    return float(np.mean(ndcgs)), float(np.mean(recalls)), len(ndcgs)\n",
        "\n",
        "\n",
        "def plot_history(hist, title_prefix=\"\"):\n",
        "    epochs = [h[\"epoch\"] for h in hist]\n",
        "    for key in [\"ndcg\", \"recall\"]:\n",
        "        vals = [h[key] for h in hist]\n",
        "        plt.figure()\n",
        "        plt.plot(epochs, vals)\n",
        "        plt.xlabel(\"epoch\")\n",
        "        plt.ylabel(key)\n",
        "        plt.title(f\"{title_prefix}{key}@K by epoch\")\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def train_lightfm_with_history(train_mat, val_mat, eval_users,\n",
        "                               item_features=None, loss=\"bpr\",\n",
        "                               no_components=64, lr=0.05, epochs=5,\n",
        "                               k=10, max_users=2000, num_threads=4,\n",
        "                               seed=42, eval_seed=None):\n",
        "\"\"\"\n",
        "eval_users: List of user indices (active_idx or sparse_idx)\n",
        "seed: Model random_state (initialization + stochastic training)\n",
        "eval_seed: Seed for subsampling users in evaluation (default = model seed)\n",
        "\"\"\"\n",
        "    if eval_seed is None:\n",
        "        eval_seed = seed\n",
        "\n",
        "    model = LightFM(\n",
        "        loss=loss,\n",
        "        no_components=no_components,\n",
        "        learning_rate=lr,\n",
        "        random_state=seed\n",
        "    )\n",
        "\n",
        "    hist = []\n",
        "    desc = \"hybrid\" if item_features is not None else \"cf\"\n",
        "\n",
        "    for ep in trange(1, epochs + 1, desc=f\"Train ({desc})\", leave=False):\n",
        "        model.fit_partial(\n",
        "            train_mat,\n",
        "            item_features=item_features,\n",
        "            epochs=1,\n",
        "            num_threads=num_threads\n",
        "        )\n",
        "\n",
        "        ndcg, rec, n_eval = evaluate_model(\n",
        "            model, train_mat, val_mat, eval_users,\n",
        "            item_features=item_features,\n",
        "            k=k, max_users=max_users,\n",
        "            seed=eval_seed, num_threads=num_threads\n",
        "        )\n",
        "        hist.append({\"epoch\": ep, \"ndcg\": ndcg, \"recall\": rec, \"n_eval_users\": n_eval})\n",
        "\n",
        "    return model, hist\n",
        "\n",
        "\n",
        "def run_h1_experiment(train_pos, val_pos, label,\n",
        "                      no_components=64, epochs=5, lr=0.05, k=10,\n",
        "                      max_users_eval=2000, num_threads=4,\n",
        "                      seed=42, top_quantile=0.7,\n",
        "                      plot=True):\n",
        "    \"\"\"\n",
        "    H1: For active users (dense mode), content does not improve ranking (ΔNDCG <= 0)\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\n================ H1: {label} (seed={seed}) ================\")\n",
        "\n",
        "    user_ids, item_ids_train, user2idx, item2idx = build_mappings(train_pos)\n",
        "    n_users, n_items = len(user_ids), len(item_ids_train)\n",
        "\n",
        "# filter validation for train mapping\n",
        "    val_pos_f = val_pos.filter(\n",
        "        pl.col(\"user_id\").is_in(pl.Series(user_ids)) &\n",
        "        pl.col(\"item_id\").is_in(pl.Series(item_ids_train))\n",
        "    )\n",
        "\n",
        "    train_mat = to_sparse_matrix(train_pos, user2idx, item2idx, n_users, n_items)\n",
        "    val_mat   = to_sparse_matrix(val_pos_f, user2idx, item2idx, n_users, n_items)\n",
        "\n",
        "    print(\"train:\", train_mat.shape, \"nnz:\", train_mat.nnz)\n",
        "    print(\"val:  \", val_mat.shape,   \"nnz:\", val_mat.nnz)\n",
        "\n",
        "    active_idx, _, thr = get_active_users(train_mat, top_quantile=top_quantile)\n",
        "    print(\"active users threshold:\", thr)\n",
        "    print(\"active users:\", len(active_idx), \"/\", n_users)\n",
        "\n",
        "    # item_features from global item_ids/item_embeddings\n",
        "    item_features = build_item_features(item_ids_train, item2idx, item_ids, item_embeddings)\n",
        "    print(\"item_features:\", item_features.shape, \"nnz:\", item_features.nnz)\n",
        "\n",
        "    # --- CF-only ---\n",
        "    model_cf, hist_cf = train_lightfm_with_history(\n",
        "        train_mat, val_mat, active_idx,\n",
        "        item_features=None,\n",
        "        loss=\"bpr\", no_components=no_components, lr=lr, epochs=epochs,\n",
        "        k=k, max_users=max_users_eval, num_threads=num_threads,\n",
        "        seed=seed, eval_seed=seed\n",
        "    )\n",
        "\n",
        "    # --- CF + content ---\n",
        "    model_h, hist_h = train_lightfm_with_history(\n",
        "        train_mat, val_mat, active_idx,\n",
        "        item_features=item_features,\n",
        "        loss=\"bpr\", no_components=no_components, lr=lr, epochs=epochs,\n",
        "        k=k, max_users=max_users_eval, num_threads=num_threads,\n",
        "        seed=seed, eval_seed=seed\n",
        "    )\n",
        "\n",
        "    ndcg_cf, rec_cf = hist_cf[-1][\"ndcg\"], hist_cf[-1][\"recall\"]\n",
        "    ndcg_h,  rec_h  = hist_h[-1][\"ndcg\"],  hist_h[-1][\"recall\"]\n",
        "\n",
        "    print(f\"CF-only    | NDCG@{k}: {ndcg_cf:.5f} | Recall@{k}: {rec_cf:.5f}\")\n",
        "    print(f\"CF+content | NDCG@{k}: {ndcg_h:.5f} | Recall@{k}: {rec_h:.5f}\")\n",
        "\n",
        "    delta = float(ndcg_h - ndcg_cf)\n",
        "    print(f\"ΔNDCG@{k}: {delta:+.5f}\")\n",
        "\n",
        "    if delta <= 0:\n",
        "        print(\"H1 is confirmed: content does not improve rankings for active users.\")\n",
        "    else:\n",
        "        print(\"H1 is not confirmed: content improves ranking.\")\n",
        "\n",
        "    if plot:\n",
        "        plot_history(hist_cf, title_prefix=f\"{label} | CF-only | \")\n",
        "        plot_history(hist_h,  title_prefix=f\"{label} | CF+content | \")\n",
        "\n",
        "    return {\n",
        "        \"label\": label,\n",
        "        \"seed\": seed,\n",
        "        \"hist_cf\": hist_cf,\n",
        "        \"hist_h\": hist_h,\n",
        "        \"ndcg_cf\": float(ndcg_cf),\n",
        "        \"rec_cf\": float(rec_cf),\n",
        "        \"ndcg_h\": float(ndcg_h),\n",
        "        \"rec_h\": float(rec_h),\n",
        "        \"delta_ndcg\": delta,\n",
        "        \"active_thr\": float(thr),\n",
        "        \"n_active\": int(len(active_idx)),\n",
        "        \"train_nnz\": int(train_mat.nnz),\n",
        "        \"val_nnz\": int(val_mat.nnz),\n",
        "        \"n_users\": int(n_users),\n",
        "        \"n_items\": int(n_items),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rlx793lFZCTg"
      },
      "source": [
        "**Launching the experiment itself**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "P1T7hQPgZDkX"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# H1: 42-SEED CHECK\n",
        "# ============================================================\n",
        "res_P1 = run_h1_experiment(\n",
        "    train_pos_P1, val_pos_P1,\n",
        "    label=f\"P1: watch_ratio >= {WATCH_THR}\",\n",
        "    epochs=5, k=10,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "res_P2 = run_h1_experiment(\n",
        "    train_pos_P2, val_pos_P2,\n",
        "    label=\"P2: like OR share OR bookmark\",\n",
        "    epochs=5, k=10,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "res_P1[\"delta_ndcg\"], res_P2[\"delta_ndcg\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# H1: MULTI-SEED CHECK (sustainability of effect)\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SEEDS = [43, 44, 45]\n",
        "EPOCHS = 5\n",
        "K = 10\n",
        "\n",
        "\n",
        "def run_h1_multiseed(train_pos, val_pos, base_label, seeds):\n",
        "    rows = []\n",
        "    histories_cf = []\n",
        "    histories_h  = []\n",
        "\n",
        "    for s in seeds:\n",
        "        res = run_h1_experiment(\n",
        "            train_pos, val_pos,\n",
        "            label=f\"{base_label} | seed={s}\",\n",
        "            epochs=EPOCHS, k=K,\n",
        "            seed=s,\n",
        "            plot=False\n",
        "        )\n",
        "\n",
        "        rows.append({\n",
        "            \"seed\": s,\n",
        "            \"ndcg_cf\": res[\"ndcg_cf\"],\n",
        "            \"ndcg_h\":  res[\"ndcg_h\"],\n",
        "            \"delta_ndcg\": res[\"delta_ndcg\"],\n",
        "        })\n",
        "\n",
        "        histories_cf.append(res[\"hist_cf\"])\n",
        "        histories_h.append(res[\"hist_h\"])\n",
        "\n",
        "    return pd.DataFrame(rows), histories_cf, histories_h\n",
        "\n",
        "\n",
        "# ---------- P1 ----------\n",
        "df_P1, hist_cf_P1, hist_h_P1 = run_h1_multiseed(\n",
        "    train_pos_P1, val_pos_P1,\n",
        "    base_label=f\"H1 / P1: watch_ratio >= {WATCH_THR}\",\n",
        "    seeds=SEEDS\n",
        ")\n",
        "\n",
        "print(\"\\n=== H1 P1 multi-seed summary ===\")\n",
        "display(df_P1)\n",
        "print(\"ΔNDCG mean:\", df_P1[\"delta_ndcg\"].mean(),\n",
        "      \"std:\", df_P1[\"delta_ndcg\"].std())\n",
        "\n",
        "\n",
        "# ---------- P2 ----------\n",
        "df_P2, hist_cf_P2, hist_h_P2 = run_h1_multiseed(\n",
        "    train_pos_P2, val_pos_P2,\n",
        "    base_label=\"H1 / P2: like | share | bookmark\",\n",
        "    seeds=SEEDS\n",
        ")\n",
        "\n",
        "print(\"\\n=== H1 P2 multi-seed summary ===\")\n",
        "display(df_P2)\n",
        "print(\"ΔNDCG mean:\", df_P2[\"delta_ndcg\"].mean(),\n",
        "      \"std:\", df_P2[\"delta_ndcg\"].std())\n",
        "\n",
        "\n",
        "# ----------  ΔNDCG ----------\n",
        "plt.figure()\n",
        "plt.bar(df_P1[\"seed\"].astype(str), df_P1[\"delta_ndcg\"])\n",
        "plt.axhline(0.0)\n",
        "plt.title(\"H1 / P1: ΔNDCG@10 by seed (Hybrid − CF)\")\n",
        "plt.xlabel(\"seed\")\n",
        "plt.ylabel(\"ΔNDCG@10\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.bar(df_P2[\"seed\"].astype(str), df_P2[\"delta_ndcg\"])\n",
        "plt.axhline(0.0)\n",
        "plt.title(\"H1 / P2: ΔNDCG@10 by seed (Hybrid − CF)\")\n",
        "plt.xlabel(\"seed\")\n",
        "plt.ylabel(\"ΔNDCG@10\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3vcTn_0q5YvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# H1: BOOTSTRAP BY USERS (confidence interval)\n",
        "# ============================================================\n",
        "def bootstrap_delta(ndcg_cf, ndcg_h, n_boot=2000, seed=123):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    deltas = ndcg_h - ndcg_cf\n",
        "    n = len(deltas)\n",
        "\n",
        "    boot_means = np.empty(n_boot)\n",
        "    for i in range(n_boot):\n",
        "        idx = rng.integers(0, n, size=n)\n",
        "        boot_means[i] = deltas[idx].mean()\n",
        "\n",
        "    mean_delta = deltas.mean()\n",
        "    ci_low, ci_high = np.quantile(boot_means, [0.025, 0.975])\n",
        "    p_improve = (boot_means > 0).mean()\n",
        "\n",
        "    return mean_delta, (ci_low, ci_high), p_improve\n",
        "\n",
        "\n",
        "# --- use one fixed seed (usually 42) ---\n",
        "BOOT_SEED = 42\n",
        "\n",
        "res_boot = run_h1_experiment(\n",
        "    train_pos_P1, val_pos_P1,\n",
        "    label=f\"H1 / P1 bootstrap | watch_ratio >= {WATCH_THR}\",\n",
        "    epochs=10, k=10,\n",
        "    seed=BOOT_SEED,\n",
        "    plot=False\n",
        ")\n",
        "\n",
        "# per-user NDCG\n",
        "ndcg_cf_users = np.array([h[\"ndcg\"] for h in res_boot[\"hist_cf\"][-1:]])\n",
        "ndcg_h_users  = np.array([h[\"ndcg\"] for h in res_boot[\"hist_h\"][-1:]])\n"
      ],
      "metadata": {
        "id": "njP6z4fD5jpG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
