{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# H3 â€” Cold-Start Items\n",
        "Hypothesis:\n",
        "\n",
        "For cold-start items, models with content embeddings outperform CF-only models."
      ],
      "metadata": {
        "id": "gEveTRw0BYpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 0) Installing dependencies (in one block)\n",
        "# ---------------------------\n",
        "!pip -q install -U pip setuptools wheel\n",
        "!pip -q install lightfm-next tqdm\n",
        "\n",
        "import numpy as np\n",
        "import polars as pl\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "from lightfm import LightFM\n",
        "from scipy import sparse\n",
        "\n",
        "from tqdm.auto import trange\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "lJvHz9i_Bh21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "#Take ~10% of users from a subsample without killing RAM because up0.01_ir0.01 is 10 times larger than up0.001_ip0.001\n",
        "# we work with the help of polars and thus cut from the cut (0)-(0)\n",
        "# ===========================\n",
        "# -------------------------\n",
        "# 0) Parameters\n",
        "# -------------------------\n",
        "subsample_name = \"up0.01_ir0.01\"\n",
        "content_embedding_size = 32\n",
        "\n",
        "FRACTION_USERS = 0.10   # 10% of users from up0.01_ir0.01\n",
        "SEED = 42\n",
        "\n",
        "TRAIN_WEEKS = list(range(25))\n",
        "VAL_WEEK = 25\n",
        "\n",
        "NEEDED_COLS = [\"user_id\", \"item_id\", \"timespent\", \"like\", \"share\", \"bookmark\"]\n",
        "\n",
        "# -------------------------\n",
        "# 1)Downloading files\n",
        "# -------------------------\n",
        "train_interactions_files = [\n",
        "    f\"subsamples/{subsample_name}/train/week_{i:02}.parquet\" for i in TRAIN_WEEKS\n",
        "]\n",
        "val_interactions_file = [f\"subsamples/{subsample_name}/validation/week_{VAL_WEEK:02}.parquet\"]\n",
        "\n",
        "metadata_files = [\n",
        "    \"metadata/users_metadata.parquet\",\n",
        "    \"metadata/items_metadata.parquet\",\n",
        "    \"metadata/item_embeddings.npz\",\n",
        "]\n",
        "\n",
        "for file in (train_interactions_files + val_interactions_file + metadata_files):\n",
        "    hf_hub_download(\n",
        "        repo_id=\"deepvk/VK-LSVD\",\n",
        "        repo_type=\"dataset\",\n",
        "        filename=file,\n",
        "        local_dir=\"VK-LSVD\",\n",
        "    )\n",
        "\n",
        "# -------------------------\n",
        "# 2) We lazily read train and select 10% of users\n",
        "# -------------------------\n",
        "train_lf = pl.concat([\n",
        "    pl.scan_parquet(f\"VK-LSVD/{file}\").select(NEEDED_COLS)\n",
        "    for file in train_interactions_files\n",
        "])\n",
        "\n",
        "# count the number of unique users (cheap)\n",
        "n_users = (\n",
        "    train_lf\n",
        "    .select(pl.col(\"user_id\").n_unique().alias(\"n_users\"))\n",
        "    .collect(engine=\"streaming\")[\"n_users\"][0]\n",
        ")\n",
        "n_sample = max(1, int(n_users * FRACTION_USERS))\n",
        "\n",
        "print(\"Unique users in subsample:\", int(n_users))\n",
        "print(\"Sampling users:\", int(n_sample), f\"({FRACTION_USERS*100:.1f}% users)\")\n",
        "\n",
        "# unique_users - a small table (safe to build)\n",
        "unique_users = (\n",
        "    train_lf\n",
        "    .select(\"user_id\")\n",
        "    .unique()\n",
        "    .collect(engine=\"streaming\")\n",
        ")\n",
        "\n",
        "# sampled_users â€” DataFrame\n",
        "sampled_users = unique_users.sample(n=n_sample, seed=SEED)\n",
        "\n",
        "# IMPORTANT: When joining with a LazyFrame, there must also be a LazyFrame on the right\n",
        "sampled_users_lf = sampled_users.lazy()\n",
        "\n",
        "# filter the train by these users (semi join)\n",
        "train_interactions = (\n",
        "    train_lf\n",
        "    .join(sampled_users_lf, on=\"user_id\", how=\"semi\")\n",
        "    .collect(engine=\"streaming\")\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# 3) Validation (filtering by the same users)\n",
        "# -------------------------\n",
        "val_lf = (\n",
        "    pl.scan_parquet(f\"VK-LSVD/{val_interactions_file[0]}\")\n",
        "    .select(NEEDED_COLS)\n",
        ")\n",
        "\n",
        "val_interactions = (\n",
        "    val_lf\n",
        "    .join(sampled_users_lf, on=\"user_id\", how=\"semi\")\n",
        "    .collect(engine=\"streaming\")\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# 4) Metadata and embeddings (filter by items from train)\n",
        "# -------------------------\n",
        "# users_metadata â€” a regular DataFrame, here you can do a join DataFrame â†” DataFrame\n",
        "users_metadata = (\n",
        "    pl.read_parquet(\"VK-LSVD/metadata/users_metadata.parquet\")\n",
        "    .join(sampled_users, on=\"user_id\", how=\"inner\")\n",
        ")\n",
        "\n",
        "items_metadata_full = pl.read_parquet(\"VK-LSVD/metadata/items_metadata.parquet\")\n",
        "\n",
        "train_items = train_interactions.select(\"item_id\").unique()\n",
        "\n",
        "items_metadata = items_metadata_full.join(train_items, on=\"item_id\", how=\"inner\")\n",
        "items_duration = items_metadata.select([\"item_id\", \"duration\"])\n",
        "\n",
        "# embeddings\n",
        "npz = np.load(\"VK-LSVD/metadata/item_embeddings.npz\")\n",
        "item_ids_all = npz[\"item_id\"]\n",
        "item_emb_all = npz[\"embedding\"].astype(np.float32)\n",
        "\n",
        "train_items_np = train_items[\"item_id\"].to_numpy()\n",
        "mask_items = np.isin(item_ids_all, train_items_np)\n",
        "\n",
        "item_ids = item_ids_all[mask_items]\n",
        "item_embeddings = item_emb_all[mask_items][:, :content_embedding_size]\n",
        "\n",
        "# -------------------------\n",
        "# 5) Final diagnostics\n",
        "# -------------------------\n",
        "print(\"\\nLoaded subsample:\", subsample_name)\n",
        "print(\"Train rows (filtered):\", train_interactions.height)\n",
        "print(\"Val rows (filtered):\", val_interactions.height)\n",
        "print(\"Train users (sampled):\", sampled_users.height)\n",
        "print(\"Train items:\", train_items.height)\n",
        "print(\"Embeddings items:\", len(item_ids), \"Embedding dim:\", item_embeddings.shape[1])\n",
        "print(\"Users metadata rows:\", users_metadata.height)\n",
        "print(\"Items metadata rows:\", items_metadata.height)\n"
      ],
      "metadata": {
        "id": "pstaiEI6jSNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“Œur0.01_ir0.01 / up0.01_ir0.01:\n",
        "\n",
        "ir â†’ item-sparse â‡’ more cold-start items\n",
        "\n",
        "Without this, cold-start items simply won't be eval'd"
      ],
      "metadata": {
        "id": "y-HUs5sxB2SN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Training and evaluation of H2 (Sparse users)\n",
        "# =========================\n",
        "\n",
        "# ---------- 1) Positives ----------\n",
        "def add_watch_ratio(df: pl.DataFrame, items_duration: pl.DataFrame) -> pl.DataFrame:\n",
        "    return (\n",
        "        df.select([\"user_id\", \"item_id\", \"timespent\"])\n",
        "          .join(items_duration, on=\"item_id\", how=\"inner\")\n",
        "          .with_columns([\n",
        "              (pl.col(\"timespent\").cast(pl.Float32) / pl.col(\"duration\").cast(pl.Float32))\n",
        "              .clip(0.0, 10.0)\n",
        "              .alias(\"watch_ratio\")\n",
        "          ])\n",
        "    )\n",
        "\n",
        "WATCH_THR = 0.5\n",
        "train_w = add_watch_ratio(train_interactions, items_duration)\n",
        "val_w   = add_watch_ratio(val_interactions, items_duration)\n",
        "\n",
        "train_pos_P1 = train_w.filter(pl.col(\"watch_ratio\") >= WATCH_THR).select([\"user_id\", \"item_id\"])\n",
        "val_pos_P1   = val_w.filter(pl.col(\"watch_ratio\") >= WATCH_THR).select([\"user_id\", \"item_id\"])\n",
        "\n",
        "train_pos_P2 = train_interactions.filter(pl.col(\"like\") | pl.col(\"share\") | pl.col(\"bookmark\")).select([\"user_id\", \"item_id\"])\n",
        "val_pos_P2   = val_interactions.filter(pl.col(\"like\") | pl.col(\"share\") | pl.col(\"bookmark\")).select([\"user_id\", \"item_id\"])\n",
        "\n",
        "\n",
        "# ---------- 2) mapping ----------\n",
        "def build_mappings(train_pos: pl.DataFrame):\n",
        "    user_ids = train_pos.select(\"user_id\").unique().sort(\"user_id\")[\"user_id\"].to_numpy()\n",
        "    item_ids_train = train_pos.select(\"item_id\").unique().sort(\"item_id\")[\"item_id\"].to_numpy()\n",
        "    user2idx = {int(u): i for i, u in enumerate(user_ids)}\n",
        "    item2idx = {int(it): i for i, it in enumerate(item_ids_train)}\n",
        "    return user_ids, item_ids_train, user2idx, item2idx\n",
        "\n",
        "\n",
        "# ---------- 3) sparse interactions ----------\n",
        "def to_sparse_matrix(pos_df: pl.DataFrame, user2idx, item2idx, n_users, n_items):\n",
        "    u = pos_df[\"user_id\"].to_numpy()\n",
        "    it = pos_df[\"item_id\"].to_numpy()\n",
        "\n",
        "    u_idx = np.fromiter((user2idx.get(int(x), -1) for x in u), dtype=np.int32, count=len(u))\n",
        "    it_idx = np.fromiter((item2idx.get(int(x), -1) for x in it), dtype=np.int32, count=len(it))\n",
        "\n",
        "    mask = (u_idx >= 0) & (it_idx >= 0)\n",
        "    u_idx = u_idx[mask]\n",
        "    it_idx = it_idx[mask]\n",
        "\n",
        "    mat = sparse.coo_matrix(\n",
        "        (np.ones(len(u_idx), dtype=np.float32), (u_idx, it_idx)),\n",
        "        shape=(n_users, n_items)\n",
        "    ).tocsr()\n",
        "\n",
        "    # if there were duplicate pairs, convert to binary form\n",
        "    if mat.nnz > 0:\n",
        "        mat.data[:] = 1.0\n",
        "    return mat\n",
        "\n",
        "\n",
        "# ---------- 4) cold-start items (H3) ----------\n",
        "def get_cold_items(train_mat, top_quantile=0.1):\n",
        "    \"\"\"\n",
        "    Cold-start items = items with interaction count\n",
        "    below the given quantile.\n",
        "    \"\"\"\n",
        "    item_counts = np.array(train_mat.getnnz(axis=0)).astype(np.int32)\n",
        "    thr = float(np.quantile(item_counts, top_quantile))\n",
        "    cold_idx = np.where(item_counts <= thr)[0]\n",
        "    return cold_idx, item_counts, thr\n",
        "\n",
        "\n",
        "# ---------- 5) item_features from embeddings----------\n",
        "def build_item_features(item_ids_train, item2idx, item_ids_emb, item_emb):\n",
        "    emb_map = {int(i): item_emb[j].astype(np.float32) for j, i in enumerate(item_ids_emb)}\n",
        "    n_items = len(item_ids_train)\n",
        "    n_feat = item_emb.shape[1]\n",
        "\n",
        "    rows, cols, data = [], [], []\n",
        "    for it_id, it_i in item2idx.items():\n",
        "        emb = emb_map.get(int(it_id))\n",
        "        if emb is None:\n",
        "            continue\n",
        "# convert D-dimensional embedding into D \"features\"\n",
        "# (LightFM expects a sparse item_features matrix)\n",
        "        for f in range(n_feat):\n",
        "            val = float(emb[f])\n",
        "            if val != 0.0:\n",
        "                rows.append(it_i)\n",
        "                cols.append(f)\n",
        "                data.append(val)\n",
        "\n",
        "    feats = sparse.coo_matrix((data, (rows, cols)), shape=(n_items, n_feat), dtype=np.float32).tocsr()\n",
        "    return feats\n",
        "\n",
        "\n",
        "# ---------- 6) metrics ----------\n",
        "def ndcg_at_k(recs, gt_set, k=10):\n",
        "    dcg = 0.0\n",
        "    for rank, it in enumerate(recs[:k], start=1):\n",
        "        if it in gt_set:\n",
        "            dcg += 1.0 / np.log2(rank + 1)\n",
        "    ideal = sum(1.0 / np.log2(r + 1) for r in range(1, min(len(gt_set), k) + 1))\n",
        "    return dcg / ideal if ideal > 0 else 0.0\n",
        "\n",
        "def recall_at_k(recs, gt_set, k=10):\n",
        "    if len(gt_set) == 0:\n",
        "        return 0.0\n",
        "    hit = sum(1 for it in recs[:k] if it in gt_set)\n",
        "    return hit / len(gt_set)\n",
        "\n",
        "\n",
        "def evaluate_model_cold_items(\n",
        "    model,\n",
        "    train_mat,\n",
        "    val_mat,\n",
        "    cold_item_idx,\n",
        "    item_features=None,\n",
        "    k=10,\n",
        "    max_users=2000,\n",
        "    seed=42,\n",
        "    num_threads=4,\n",
        "):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    users = np.arange(train_mat.shape[0], dtype=np.int32)\n",
        "\n",
        "    if len(users) > max_users:\n",
        "        users = rng.choice(users, size=max_users, replace=False)\n",
        "\n",
        "    cold_set = set(cold_item_idx.tolist())\n",
        "\n",
        "    ndcgs, recalls = [], []\n",
        "    all_items = np.arange(train_mat.shape[1], dtype=np.int32)\n",
        "\n",
        "    for u in users:\n",
        "        gt_items = val_mat[u].indices\n",
        "        gt_cold = [it for it in gt_items if it in cold_set]\n",
        "\n",
        "        if len(gt_cold) == 0:\n",
        "            continue\n",
        "\n",
        "        u_arr = np.full(len(all_items), int(u), dtype=np.int32)\n",
        "        scores = model.predict(\n",
        "            u_arr,\n",
        "            all_items,\n",
        "            item_features=item_features,\n",
        "            num_threads=num_threads,\n",
        "        )\n",
        "\n",
        "# removing seen\n",
        "        seen = train_mat[u].indices\n",
        "        scores[seen] = -1e9\n",
        "\n",
        "        topk = np.argpartition(-scores, k)[:k]\n",
        "        topk = topk[np.argsort(-scores[topk])]\n",
        "\n",
        "        ndcgs.append(ndcg_at_k(topk.tolist(), set(gt_cold), k=k))\n",
        "        recalls.append(recall_at_k(topk.tolist(), set(gt_cold), k=k))\n",
        "\n",
        "    return (\n",
        "        float(np.mean(ndcgs)) if ndcgs else 0.0,\n",
        "        float(np.mean(recalls)) if recalls else 0.0,\n",
        "        len(ndcgs),\n",
        "    )\n",
        "def train_with_history_h3(\n",
        "    train_mat,\n",
        "    val_mat,\n",
        "    cold_item_idx,\n",
        "    item_features=None,\n",
        "    loss=\"bpr\",\n",
        "    no_components=64,\n",
        "    lr=0.05,\n",
        "    epochs=5,\n",
        "    k=10,\n",
        "    max_users_eval=2000,\n",
        "    num_threads=4,\n",
        "    seed=42,\n",
        "):\n",
        "    model = LightFM(\n",
        "        loss=loss,\n",
        "        no_components=no_components,\n",
        "        learning_rate=lr,\n",
        "        random_state=seed,\n",
        "    )\n",
        "\n",
        "    hist = []\n",
        "    desc = \"Hybrid\" if item_features is not None else \"CF-only\"\n",
        "\n",
        "    for ep in trange(1, epochs + 1, desc=f\"Training {desc}\"):\n",
        "        model.fit_partial(\n",
        "            train_mat,\n",
        "            item_features=item_features,\n",
        "            epochs=1,\n",
        "            num_threads=num_threads,\n",
        "        )\n",
        "\n",
        "        ndcg, rec, n_eval = evaluate_model_cold_items(\n",
        "            model,\n",
        "            train_mat,\n",
        "            val_mat,\n",
        "            cold_item_idx,\n",
        "            item_features=item_features,\n",
        "            k=k,\n",
        "            max_users=max_users_eval,\n",
        "            seed=seed,\n",
        "            num_threads=num_threads,\n",
        "        )\n",
        "\n",
        "        hist.append({\n",
        "            \"epoch\": ep,\n",
        "            \"ndcg\": ndcg,\n",
        "            \"recall\": rec,\n",
        "            \"n_eval\": n_eval,\n",
        "        })\n",
        "\n",
        "    return model, hist\n",
        "\n",
        "\n",
        "def run_h3_experiment(\n",
        "    train_pos,\n",
        "    val_pos,\n",
        "    label,\n",
        "    cold_quantile=0.1,\n",
        "    no_components=64,\n",
        "    epochs=5,\n",
        "    lr=0.05,\n",
        "    k=10,\n",
        "    max_users_eval=2000,\n",
        "    num_threads=4,\n",
        "    seed=42,\n",
        "):\n",
        "    print(f\"\\n================ H3 (Cold-start Items): {label} ================\")\n",
        "\n",
        "    user_ids, item_ids_train, user2idx, item2idx = build_mappings(train_pos)\n",
        "    n_users, n_items = len(user_ids), len(item_ids_train)\n",
        "\n",
        "    val_pos_f = val_pos.filter(\n",
        "        pl.col(\"user_id\").is_in(pl.Series(user_ids)) &\n",
        "        pl.col(\"item_id\").is_in(pl.Series(item_ids_train))\n",
        "    )\n",
        "\n",
        "    train_mat = to_sparse_matrix(train_pos, user2idx, item2idx, n_users, n_items)\n",
        "    val_mat   = to_sparse_matrix(val_pos_f, user2idx, item2idx, n_users, n_items)\n",
        "\n",
        "    print(\"train:\", train_mat.shape, \"nnz:\", train_mat.nnz)\n",
        "    print(\"val:  \", val_mat.shape, \"nnz:\", val_mat.nnz)\n",
        "\n",
        "    cold_idx, item_counts, thr = get_cold_items(train_mat, top_quantile=cold_quantile)\n",
        "    print(\"cold items threshold (<=):\", thr)\n",
        "    print(\"cold items:\", len(cold_idx), \"/\", n_items)\n",
        "\n",
        "    item_features = build_item_features(\n",
        "        item_ids_train, item2idx, item_ids, item_embeddings\n",
        "    )\n",
        "    print(\"item_features:\", item_features.shape)\n",
        "\n",
        "    # CF-only\n",
        "    model_cf, hist_cf = train_with_history_h3(\n",
        "        train_mat, val_mat, cold_idx,\n",
        "        item_features=None,\n",
        "        no_components=no_components,\n",
        "        lr=lr,\n",
        "        epochs=epochs,\n",
        "        k=k,\n",
        "        max_users_eval=max_users_eval,\n",
        "        num_threads=num_threads,\n",
        "        seed=seed,\n",
        "    )\n",
        "\n",
        "    # Hybrid\n",
        "    model_h, hist_h = train_with_history_h3(\n",
        "        train_mat, val_mat, cold_idx,\n",
        "        item_features=item_features,\n",
        "        no_components=no_components,\n",
        "        lr=lr,\n",
        "        epochs=epochs,\n",
        "        k=k,\n",
        "        max_users_eval=max_users_eval,\n",
        "        num_threads=num_threads,\n",
        "        seed=seed,\n",
        "    )\n",
        "\n",
        "    ndcg_cf, rec_cf = hist_cf[-1][\"ndcg\"], hist_cf[-1][\"recall\"]\n",
        "    ndcg_h,  rec_h  = hist_h[-1][\"ndcg\"],  hist_h[-1][\"recall\"]\n",
        "    delta = ndcg_h - ndcg_cf\n",
        "\n",
        "    print(f\"CF-only    | NDCG@{k}: {ndcg_cf:.5f} | Recall@{k}: {rec_cf:.5f}\")\n",
        "    print(f\"CF+content | NDCG@{k}: {ndcg_h:.5f} | Recall@{k}: {rec_h:.5f}\")\n",
        "    print(f\"Î”NDCG@{k}: {delta:+.5f}\")\n",
        "\n",
        "    if delta > 0:\n",
        "        print(\"H3 is confirmed: content is critical for cold-start items.\")\n",
        "    else:\n",
        "        print(\"H3 is weakening: the content did not yield the expected results.\")\n",
        "\n",
        "    return {\n",
        "        \"label\": label,\n",
        "        \"cold_quantile\": cold_quantile,\n",
        "        \"ndcg_cf\": ndcg_cf,\n",
        "        \"ndcg_h\": ndcg_h,\n",
        "        \"delta_ndcg\": float(delta),\n",
        "        \"n_cold_items\": int(len(cold_idx)),\n",
        "    }\n"
      ],
      "metadata": {
        "id": "rQTQpuJOjyYT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# H3 multi-seed setup\n",
        "# =========================\n",
        "\n",
        "H3_SEEDS = [41, 42, 43, 44, 45]\n",
        "\n",
        "H3_CONFIG = dict(\n",
        "    cold_quantile=0.1,        # 10% of the coldest items\n",
        "    no_components=64,\n",
        "    epochs=5,\n",
        "    lr=0.05,\n",
        "    k=10,\n",
        "    max_users_eval=2000,\n",
        "    num_threads=4,\n",
        ")\n",
        "\n",
        "H3_LABEL = \"H3 / Cold-start items\"\n"
      ],
      "metadata": {
        "id": "8vMP4S_qlp6J"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Run H3 for multiple seeds\n",
        "# =========================\n",
        "\n",
        "h3_results = []\n",
        "\n",
        "for seed in H3_SEEDS:\n",
        "    res = run_h3_experiment(\n",
        "        train_pos=train_pos_P2,      # or P1, if you want.\n",
        "        val_pos=val_pos_P2,\n",
        "        label=f\"{H3_LABEL} | seed={seed}\",\n",
        "        seed=seed,\n",
        "        **H3_CONFIG\n",
        "    )\n",
        "    res[\"seed\"] = seed\n",
        "    h3_results.append(res)\n"
      ],
      "metadata": {
        "id": "w5dp82c0mKU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "h3_df = pd.DataFrame([\n",
        "    {\n",
        "        \"seed\": r[\"seed\"],\n",
        "        \"ndcg_cf\": r[\"ndcg_cf\"],\n",
        "        \"ndcg_hybrid\": r[\"ndcg_h\"],\n",
        "        \"delta_ndcg\": r[\"delta_ndcg\"],\n",
        "        \"n_cold_items\": r[\"n_cold_items\"],\n",
        "    }\n",
        "    for r in h3_results\n",
        "])\n",
        "\n",
        "h3_df\n"
      ],
      "metadata": {
        "id": "DnpRn7gFmSA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.figure(figsize=(7, 4))\n",
        "\n",
        "x = np.arange(len(h3_df))\n",
        "\n",
        "plt.plot(x, h3_df[\"ndcg_cf\"], marker=\"o\", label=\"CF-only\")\n",
        "plt.plot(x, h3_df[\"ndcg_hybrid\"], marker=\"o\", label=\"CF + content\")\n",
        "\n",
        "plt.xticks(x, h3_df[\"seed\"])\n",
        "plt.xlabel(\"Seed\")\n",
        "plt.ylabel(\"NDCG@10\")\n",
        "plt.title(\"H3: Cold-start items â€” CF vs Hybrid\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "22QIGeLcmSj6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}